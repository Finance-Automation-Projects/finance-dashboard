{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and Tables RAG on Infosys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./annual_reports/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyushkant/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing PDF:   0%|          | 0/229 pages [31:21<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Get total pages first\n",
    "doc = fitz.open(path + \"Hindunilvr.pdf\")\n",
    "total_pages = len(doc)\n",
    "doc.close()\n",
    "\n",
    "# Create a callback function for the progress bar with a better format\n",
    "pbar = tqdm(\n",
    "    total=total_pages,\n",
    "    desc=\"Processing PDF\",\n",
    "    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} pages [{elapsed}<{remaining}, {rate_fmt}]'\n",
    ")\n",
    "\n",
    "def progress_callback(page_num: int, total: int):\n",
    "    pbar.update(1)\n",
    "\n",
    "try:\n",
    "    # Get elements\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=path + \"Hindunilvr.pdf\",\n",
    "        # Unstructured first finds embedded image blocks\n",
    "        extract_images_in_pdf=False,\n",
    "        # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "        # Titles are any sub-section of the document\n",
    "        infer_table_structure=True,\n",
    "        # Post processing to aggregate text once we have the title\n",
    "        chunking_strategy=\"by_title\",\n",
    "        # Chunking params to aggregate text blocks\n",
    "        # Attempt to create a new chunk 3800 chars\n",
    "        # Attempt to keep chunks > 2000 chars\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=path,\n",
    "        progress_bar=progress_callback,\n",
    "        include_page_breaks=True\n",
    "    )\n",
    "finally:\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 519,\n",
       " \"<class 'unstructured.documents.elements.TableChunk'>\": 7,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivector retrievar\n",
    "\n",
    "### Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an financial assistant tasked with summarizing tables and text. \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatGroq(temperature=0, model=\"llama3-groq-8b-8192-tool-use-preview\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "import asyncio\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import List\n",
    "import math\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class GroqBatcher:\n",
    "    def __init__(self, \n",
    "                 batch_size: int = 5,\n",
    "                 requests_per_minute: int = 25,  # Conservative limit under 30\n",
    "                 tokens_per_minute: int = 14000, # Conservative under 15000\n",
    "                 tokens_per_day: int = 450000,   # Conservative under 500000\n",
    "                 base_delay: float = 2.0):\n",
    "        self.batch_size = batch_size\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "        self.tokens_per_day = tokens_per_day\n",
    "        self.base_delay = base_delay\n",
    "        self.request_times = deque()\n",
    "        self.token_usage = deque()\n",
    "        self.daily_token_usage = 0\n",
    "        self.reset_time = time.time()\n",
    "\n",
    "    async def process_batch(self, items: List[str], summarize_chain) -> List[str]:\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(items), self.batch_size):\n",
    "            batch = items[i:i + self.batch_size]\n",
    "            batch_results = await self._process_items(batch, summarize_chain)\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # Add delay between batches\n",
    "            await asyncio.sleep(self.base_delay)\n",
    "            \n",
    "        return results\n",
    "\n",
    "    async def _process_items(self, batch: List[str], summarize_chain) -> List[str]:\n",
    "        results = []\n",
    "        retry_count = 0\n",
    "        \n",
    "        for item in batch:\n",
    "            try:\n",
    "                now = time.time()\n",
    "                \n",
    "                # Reset daily counts if 24 hours passed\n",
    "                if now - self.reset_time > 86400:\n",
    "                    self.daily_token_usage = 0\n",
    "                    self.reset_time = now\n",
    "                \n",
    "                # Clean up old records\n",
    "                while self.request_times and now - self.request_times[0] > 60:\n",
    "                    self.request_times.popleft()\n",
    "                    self.token_usage.popleft()\n",
    "\n",
    "                # Check rate limits\n",
    "                if len(self.request_times) >= self.requests_per_minute:\n",
    "                    wait_time = 60 - (now - self.request_times[0])\n",
    "                    await asyncio.sleep(max(0, wait_time))\n",
    "\n",
    "                if sum(self.token_usage) >= self.tokens_per_minute:\n",
    "                    wait_time = 60 - (now - self.request_times[0])\n",
    "                    await asyncio.sleep(max(0, wait_time))\n",
    "\n",
    "                # Process item\n",
    "                result = await summarize_chain.ainvoke(item)\n",
    "                \n",
    "                # Update tracking\n",
    "                self.request_times.append(time.time())\n",
    "                tokens_used = len(item.split()) + len(str(result).split())\n",
    "                self.token_usage.append(tokens_used)\n",
    "                self.daily_token_usage += tokens_used\n",
    "                \n",
    "                results.append(result)\n",
    "                retry_count = 0  # Reset retry count on success\n",
    "\n",
    "            except Exception as e:\n",
    "                if \"rate_limit_exceeded\" in str(e):\n",
    "                    retry_count += 1\n",
    "                    delay = self.base_delay * math.exp(retry_count)\n",
    "                    await asyncio.sleep(delay)\n",
    "                    continue\n",
    "                raise e\n",
    "\n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "async def process_tables(tables: List[str], summarize_chain) -> List[str]:\n",
    "    batcher = GroqBatcher(\n",
    "        batch_size=5,\n",
    "        requests_per_minute=25\n",
    "    )\n",
    "    return await batcher.process_batch(tables, summarize_chain)\n",
    "\n",
    "# Execute\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Replace your original code with:\n",
    "table_summaries = asyncio.get_event_loop().run_until_complete(\n",
    "    process_tables(tables, summarize_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import time\n",
    "# from collections import deque\n",
    "# from typing import List\n",
    "# import math\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# class TextBatchProcessor:\n",
    "#     def __init__(self, \n",
    "#                  batch_size: int = 5,\n",
    "#                  requests_per_minute: int = 25,  # Under 30 limit\n",
    "#                  tokens_per_minute: int = 14000, # Under 15000 limit\n",
    "#                  tokens_per_day: int = 450000,   # Under 500000 limit\n",
    "#                  max_concurrency: int = 5,\n",
    "#                  base_delay: float = 2.0):\n",
    "#         self.batch_size = batch_size\n",
    "#         self.requests_per_minute = requests_per_minute\n",
    "#         self.tokens_per_minute = tokens_per_minute\n",
    "#         self.tokens_per_day = tokens_per_day\n",
    "#         self.max_concurrency = max_concurrency\n",
    "#         self.base_delay = base_delay\n",
    "#         self.request_times = deque()\n",
    "#         self.token_usage = deque()\n",
    "#         self.daily_token_usage = 0\n",
    "#         self.reset_time = time.time()\n",
    "\n",
    "#     async def process_texts(self, texts: List[str], summarize_chain) -> List[str]:\n",
    "#         summaries = []\n",
    "#         semaphore = asyncio.Semaphore(self.max_concurrency)\n",
    "        \n",
    "#         async def process_with_rate_limit(text: str) -> str:\n",
    "#             async with semaphore:\n",
    "#                 try:\n",
    "#                     now = time.time()\n",
    "                    \n",
    "#                     # Reset daily tracking if needed\n",
    "#                     if now - self.reset_time > 86400:\n",
    "#                         self.daily_token_usage = 0\n",
    "#                         self.reset_time = now\n",
    "                    \n",
    "#                     # Clean up old records\n",
    "#                     while self.request_times and now - self.request_times[0] > 60:\n",
    "#                         self.request_times.popleft()\n",
    "#                         self.token_usage.popleft()\n",
    "\n",
    "#                     # Check rate limits\n",
    "#                     if len(self.request_times) >= self.requests_per_minute:\n",
    "#                         wait_time = 60 - (now - self.request_times[0])\n",
    "#                         await asyncio.sleep(max(0, wait_time))\n",
    "\n",
    "#                     if sum(self.token_usage) >= self.tokens_per_minute:\n",
    "#                         wait_time = 60 - (now - self.request_times[0])\n",
    "#                         await asyncio.sleep(max(0, wait_time))\n",
    "\n",
    "#                     # Process text\n",
    "#                     summary = await summarize_chain.ainvoke(text)\n",
    "                    \n",
    "#                     # Update tracking\n",
    "#                     self.request_times.append(time.time())\n",
    "#                     tokens_used = len(text.split()) + len(str(summary).split())\n",
    "#                     self.token_usage.append(tokens_used)\n",
    "#                     self.daily_token_usage += tokens_used\n",
    "                    \n",
    "#                     return summary\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     if \"rate_limit_exceeded\" in str(e):\n",
    "#                         # Parse retry delay handling both seconds and milliseconds\n",
    "#                         retry_str = str(e).split(\"try again in \")[-1].split(\" \")[0]\n",
    "#                         if retry_str.endswith('ms'):\n",
    "#                             retry_after = float(retry_str.rstrip('ms')) / 1000\n",
    "#                         else:\n",
    "#                             retry_after = float(retry_str.rstrip('s'))\n",
    "#                         await asyncio.sleep(retry_after + 0.5)\n",
    "#                         return await process_with_rate_limit(text)\n",
    "#                     raise e\n",
    "#         for i in range(0, len(texts), self.batch_size):\n",
    "#             batch = texts[i:i + self.batch_size]\n",
    "#             batch_tasks = [process_with_rate_limit(text) for text in batch]\n",
    "#             batch_results = await asyncio.gather(*batch_tasks)\n",
    "#             summaries.extend(batch_results)\n",
    "#             await asyncio.sleep(self.base_delay)\n",
    "\n",
    "#         return summaries\n",
    "\n",
    "# # Usage\n",
    "# async def process_text_summaries(texts: List[str], summarize_chain) -> List[str]:\n",
    "#     processor = TextBatchProcessor()\n",
    "#     return await processor.process_texts(texts, summarize_chain)\n",
    "\n",
    "# # Initialize and run\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Replace original code with:\n",
    "# texts = [i.text for i in text_elements]\n",
    "# text_summaries = asyncio.get_event_loop().run_until_complete(\n",
    "#     process_text_summaries(texts, summarize_chain)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-mpnet-base-v2\",\n",
    "    model_kwargs={'trust_remote_code': True}\n",
    ")\n",
    "persistent_client = chromadb.PersistentClient(\"./database\")\n",
    "collection = persistent_client.get_or_create_collection(\"rag_on_infosys_report\")\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(client=persistent_client, collection_name=\"summaries\", embedding_function=embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"You are a financial agent whose role is to get key insights from the annual report of a company realted to the User's Question. You must be very detailed in your response and must stick to the resources provided (Context here are the resources provided), now answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatGroq(temperature=0, model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the context provided, it appears to be a jumbled collection of characters, symbols, and some numerical values, which does not resemble a standard annual financial report of a company. The text lacks coherence, and there are no clear financial statements, such as balance sheets, income statements, or cash flow statements, that are typically found in an annual report.\n",
      "\n",
      "However, as a financial expert and market analyst, I can provide a general analysis based on the assumption that the context might contain some hidden or encoded financial information. \n",
      "\n",
      "1. **Lack of Transparency**: The primary concern with the provided context is its lack of transparency. Investors and analysts rely heavily on clear, concise, and detailed financial information to assess a company's performance and potential. The current state of the context does not allow for such an assessment.\n",
      "\n",
      "2. **Risk Assessment**: Without clear financial data, it's challenging to assess the company's risk profile. Typically, investors look at debt-to-equity ratios, current ratios, and other financial metrics to understand a company's financial health and risk.\n",
      "\n",
      "3. **Growth Potential**: The growth potential of a company is usually evaluated by looking at its historical revenue growth, profit margins, and investments in research and development. None of this information is discernible from the provided context.\n",
      "\n",
      "4. **Market Position**: Understanding a company's market position involves analyzing its market share, competitive landscape, and industry trends. This information is not available in the context provided.\n",
      "\n",
      "5. **Investment Potential**: The investment potential of a company is determined by its financial performance, growth prospects, industry outlook, and management's strategy. Given the lack of clear financial and operational data, it's not possible to accurately assess the company's investment potential based on the provided context.\n",
      "\n",
      "In conclusion, due to the nature of the context provided, which does not contain recognizable or usable financial information, it is not possible to provide a precise summary of the company's performance or its investment potential. For a comprehensive analysis, access to a standard, detailed, and coherent annual financial report is necessary. \n",
      "\n",
      "Recommendations for investors or potential investors would include seeking out complete and accurate financial reports, possibly through the company's official website, financial databases, or regulatory filings. Additionally, consulting with financial advisors or conducting thorough market research could provide more insights into the company's standing and prospects.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"\"\"You are a financial expert and market analyst. Analyze the annual financial report \n",
    "            of the company and provide a precise summary of the company's performance and investment potential.\n",
    "            \n",
    "            Retrieved Context:\n",
    "            {context}\n",
    "            \n",
    "            If the context is limited or missing, focus on providing general analysis based on the available information.\"\"\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
