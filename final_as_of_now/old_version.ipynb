{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Version as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" scikit-learn langgraph tavily-python bs4 firebase-admin langchain-groq langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yfinance in /home/rohitrj/.local/lib/python3.10/site-packages (0.2.48)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: requests>=2.31 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (5.3.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (3.17.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (2024.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/rohitrj/.local/lib/python3.10/site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/rohitrj/.local/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/rohitrj/.local/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rohitrj/.local/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rohitrj/.local/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.31->yfinance) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.31->yfinance) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.31->yfinance) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rohitrj/.local/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Table created with columns for each aspect with this query: \n",
      "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "            stock_symbol TEXT,\n",
      "            headline TEXT,\n",
      "            published_date TEXT,\n",
      "            url TEXT,\n",
      "            embedding BLOB,\n",
      "        Earnings REAL,\n",
      "Revenue REAL,\n",
      "Margins REAL,\n",
      "Dividend REAL,\n",
      "EBITDA REAL,\n",
      "Debt REAL,\n",
      "Sentiment REAL\n",
      "\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM stock_news_results': no such table: stock_news_results",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: stock_news_results",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 800\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Equity Research Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequity_research_report\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReport generation failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1608\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1607\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1610\u001b[0m     config,\n\u001b[1;32m   1611\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1612\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1613\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1614\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1615\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1617\u001b[0m ):\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1619\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1336\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1331\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1332\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1333\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1334\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1335\u001b[0m     ):\n\u001b[0;32m-> 1336\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1337\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1338\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1339\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1340\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1341\u001b[0m         ):\n\u001b[1;32m   1342\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/pregel/runner.py:58\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     56\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[3], line 718\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_sentiment\u001b[39m(state):\n\u001b[1;32m    717\u001b[0m     ticker \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 718\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mNewsAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     analysis_result \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalyze(ticker)\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: analysis_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mNewsAnalyzer.__init__\u001b[0;34m(self, model_name, groq_api_key, tavily_api_key)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[1;32m    110\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mgroq_api_key,   \n\u001b[1;32m    111\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    112\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.2-90b-text-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtavily_client \u001b[38;5;241m=\u001b[39m TavilyClient(api_key\u001b[38;5;241m=\u001b[39mtavily_api_key \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAVILY_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_db \u001b[38;5;241m=\u001b[39m \u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNewsDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AIDS_project/final_as_of_now/interface.py:38\u001b[0m, in \u001b[0;36mNewsDatabase.__init__\u001b[0;34m(self, db_name1, db_name2)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize the SentimentAnalyser with the specified aspects\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Create the table with columns for each aspect\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_table()\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AIDS_project/final_as_of_now/interface.py:91\u001b[0m, in \u001b[0;36mNewsDatabase.to_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql_query(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection)\n\u001b[1;32m     89\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM stock_news_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf2\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2729\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM stock_news_results': no such table: stock_news_results"
     ]
    }
   ],
   "source": [
    "### 1. LLM Setup\n",
    "import firebase_admin\n",
    "from firebase_admin import firestore,credentials\n",
    "import os\n",
    "from langchain.schema import HumanMessage\n",
    "from tavily import TavilyClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import interface\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "from langchain_groq import ChatGroq\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "# Initialize the main LLM\n",
    "# local_llm = \"llama3.2:3b-instruct-q3_K_S\"\n",
    "# llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# Suppress all UserWarnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Or suppress specific warning messages\n",
    "warnings.filterwarnings('ignore', message=\"Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\")\n",
    "### 2. Document Loaders and Vector Stores\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "### 3. Agent Prompts\n",
    "\n",
    "GROQ_API_KEY = \"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\"  # Replace with your actual API key\n",
    "\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    api_key = GROQ_API_KEY,\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.2-90b-text-preview\"  \n",
    ")\n",
    "# Report Generator\n",
    "report_generator_prompt = \"\"\"You are a senior equity research analyst tasked with creating a comprehensive investment report.\n",
    "\n",
    "You have been provided with two key analyses:\n",
    "\n",
    "1. Annual Report Analysis:\n",
    "{annual_report_summary}\n",
    "\n",
    "2. Market Sentiment and News Analysis:\n",
    "{sentiment_analysis}\n",
    "\n",
    "3. Company Overview:\n",
    "{company_overview_report}\n",
    "\n",
    "4. Peer Comparison Report:\n",
    "{peer_comparison_report}\n",
    "\n",
    "Based on these inputs, generate a detailed equity research report for {ticker}. Your report should include:\n",
    "\n",
    "1. Executive Summary:\n",
    "- Clear BUY, SELL, or HOLD recommendation with target price range\n",
    "- Key investment highlights\n",
    "- Primary risks\n",
    "\n",
    "2. Financial Analysis (from Annual Report):\n",
    "- Revenue and profit trends\n",
    "- Balance sheet strength\n",
    "- Cash flow analysis\n",
    "- Key ratios and metrics\n",
    "- Management effectiveness\n",
    "\n",
    "3. Market Position & Sentiment:\n",
    "- Recent developments and their impact\n",
    "- Market perception\n",
    "- Competitive position\n",
    "- News flow analysis\n",
    "- Sentiment trends\n",
    "\n",
    "4. Risk Assessment:\n",
    "- Business risks\n",
    "- Financial risks\n",
    "- Market risks\n",
    "- Regulatory risks\n",
    "\n",
    "5. Investment Thesis:\n",
    "- Growth catalysts\n",
    "- Competitive advantages\n",
    "- Valuation perspective\n",
    "- Timeline for expected developments\n",
    "\n",
    "6. Forward Looking Assessment:\n",
    "- Short-term outlook (6-12 months)\n",
    "- Long-term prospects (2-3 years)\n",
    "- Key metrics to monitor\n",
    "\n",
    "Ensure your analysis is data-driven, balanced, and provides clear rationale for all conclusions.\"\"\"\n",
    "\n",
    "### 4. Agent Functions\n",
    "\n",
    "# [NewsAnalyzer class implementation remains the same as before]\n",
    "class NewsAnalyzer:\n",
    "    def __init__(self, model_name: str = \"llama-3.2-90b-text-preview\", groq_api_key:str = GROQ_API_KEY, tavily_api_key: str = \"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\"):\n",
    "        \"\"\"Initialize the news analyzer with LLM and Tavily API.\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=groq_api_key,   \n",
    "            temperature=0,\n",
    "            model_name=\"llama-3.2-90b-text-preview\"\n",
    "        )\n",
    "        self.tavily_client = TavilyClient(api_key=tavily_api_key or os.getenv(\"TAVILY_API_KEY\"))\n",
    "        self.news_db = interface.NewsDatabase()\n",
    "\n",
    "    def get_headlines(self, ticker: str, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Get and sort headlines for a specific ticker from the database.\"\"\"\n",
    "        try:\n",
    "            # Append '.NS' to the ticker\n",
    "            ticker_with_suffix = ticker + '.NS'\n",
    "            \n",
    "            # Get dataframe from news_db and ensure it's a DataFrame\n",
    "            df_data = self.news_db.to_dataframe()\n",
    "            if isinstance(df_data, tuple):\n",
    "                # If it's a tuple, take the first element assuming it's the DataFrame\n",
    "                df = df_data[0] if df_data else pd.DataFrame()\n",
    "            else:\n",
    "                df = df_data\n",
    "\n",
    "            if df.empty:\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            # Filter for ticker\n",
    "            ticker_df = df[df['stock_symbol'] == ticker_with_suffix].copy()\n",
    "            \n",
    "            # Convert published_date to datetime\n",
    "            if 'published_date' in ticker_df.columns:\n",
    "                ticker_df['published_date'] = pd.to_datetime(ticker_df['published_date'])\n",
    "            \n",
    "            # Check required columns\n",
    "            if 'title' not in ticker_df.columns or 'url' not in ticker_df.columns:\n",
    "                print(\"Required columns are missing from the DataFrame.\")\n",
    "                print(f\"Available columns: {ticker_df.columns}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Sort by date and get the most recent articles\n",
    "            sorted_df = ticker_df.sort_values('published_date', ascending=False).head(limit)\n",
    "            return sorted_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_headlines: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def get_article_context(self, headline: str, url: str) -> str:\n",
    "        \"\"\"Get additional context about the headline using Tavily API.\"\"\"\n",
    "        try:\n",
    "            search_result = self.tavily_client.search(\n",
    "                query=f\"Context and implications of: {headline}\",\n",
    "                search_depth=\"advanced\",\n",
    "                max_results=2\n",
    "            )\n",
    "            \n",
    "            # Check the type of search_result\n",
    "            if isinstance(search_result, dict):\n",
    "                # If search_result is a dictionary\n",
    "                context = search_result.get('content', 'No additional context found.')\n",
    "            elif isinstance(search_result, list):\n",
    "                # If search_result is a list\n",
    "                contexts = []\n",
    "                for result in search_result:\n",
    "                    if isinstance(result, dict):\n",
    "                        content = result.get('content')\n",
    "                        if content:\n",
    "                            contexts.append(content)\n",
    "                context = \" \".join(contexts) if contexts else \"No additional context found.\"\n",
    "            elif isinstance(search_result, str):\n",
    "                # If search_result is a string\n",
    "                context = search_result\n",
    "            else:\n",
    "                context = \"No additional context found.\"\n",
    "            \n",
    "            return context\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching context: {e}\")\n",
    "            return \"Unable to fetch additional context.\"\n",
    "\n",
    "    def format_date(self, date: pd.Timestamp) -> str:\n",
    "        \"\"\"Convert datetime to a readable format.\"\"\"\n",
    "        return date.strftime('%B %d, %Y')\n",
    "    \n",
    "    def get_financial_metrics(self, row: pd.Series) -> str:\n",
    "        \"\"\"Format non-zero financial metrics into a readable string.\"\"\"\n",
    "        metrics = ['Earnings', 'Revenue', 'Margins', 'Dividend', 'EBITDA', 'Debt']\n",
    "        non_zero_metrics = {metric: row[metric] for metric in metrics if row[metric] != 0}\n",
    "        if not non_zero_metrics:\n",
    "            return \"No financial metrics reported\"\n",
    "        \n",
    "        return '\\n'.join([f\"- {metric}: {value}\" for metric, value in non_zero_metrics.items()])\n",
    "\n",
    "    def format_headlines_for_prompt(self, headlines_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Format headlines and their data into a readable string for the prompt.\"\"\"\n",
    "        formatted_headlines = []\n",
    "        \n",
    "        for idx, row in headlines_df.iterrows():\n",
    "            context = self.get_article_context(row['title'], row['url'])\n",
    "            financial_metrics = self.get_financial_metrics(row)\n",
    "            \n",
    "            headline_text = f\"\"\"Headline {len(formatted_headlines) + 1}:\n",
    "    Date: {self.format_date(row['published_date'])}\n",
    "    Title: {row['title']}\n",
    "    Sentiment Score: {row['Sentiment']}\n",
    "    Financial Metrics:\n",
    "    {financial_metrics}\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "            formatted_headlines.append(headline_text)\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_headlines)\n",
    "\n",
    "    def generate_analysis(self, headlines_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate a comprehensive analysis using the LLM.\"\"\"\n",
    "        if headlines_df.empty:\n",
    "            return \"No recent headlines found for this ticker.\"\n",
    "            \n",
    "        formatted_headlines = self.format_headlines_for_prompt(headlines_df)\n",
    "        sentiment_scores = headlines_df['Sentiment'].tolist()\n",
    "\n",
    "        prompt = f\"\"\"As a stock market analyst, analyze these recent headlines and associated financial metrics for a company. For each headline, evaluate its implications for the company's future performance and market position.\n",
    "\n",
    "Latest Headlines and Data (from most recent to oldest):\n",
    "\n",
    "{formatted_headlines}\n",
    "\n",
    "Please provide a comprehensive analysis that includes:\n",
    "\n",
    "1. Latest Development Analysis:\n",
    "- Analyze the most recent headline in detail\n",
    "- Evaluate any associated financial metrics and their implications\n",
    "- Explain whether this development is BULLISH, BEARISH, or NEUTRAL for the company\n",
    "- Support your assessment with context and available financial data\n",
    "\n",
    "2. Financial Metrics Analysis:\n",
    "- Analyze any changes in key financial metrics (Earnings, Revenue, Margins, etc.)\n",
    "- Identify trends or patterns in the financial data\n",
    "- Explain how these metrics support or contradict the narrative from headlines\n",
    "\n",
    "3. Trend Analysis:\n",
    "- Compare the nature of news across all headlines\n",
    "- Identify if there's an improvement or deterioration in company developments\n",
    "- Note any pattern in the types of news (operational, strategic, market-related)\n",
    "\n",
    "4. Market Sentiment Evolution:\n",
    "- Analyze the sentiment scores: {sentiment_scores}\n",
    "- Explain if the news trajectory suggests strengthening or weakening market position\n",
    "- Note any divergence between sentiment scores and actual news/financial impact\n",
    "\n",
    "5. Forward Looking Assessment:\n",
    "- Based on these developments and metrics, provide a brief outlook\n",
    "- Highlight key areas to monitor going forward\n",
    "- Identify potential risks and opportunities based on the available data\n",
    "\n",
    "Keep the analysis evidence-based and focused on both qualitative news impact and quantitative financial metrics.\"\"\"\n",
    "\n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        #print(\"SENTIMENT ANALYSIS\",response.content)\n",
    "        return response.content\n",
    "    \n",
    "    def analyze(self, ticker: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis function that processes headlines and returns results.\"\"\"\n",
    "        # Get and process headlines\n",
    "        recent_headlines = self.get_headlines(ticker)\n",
    "        \n",
    "        if recent_headlines.empty:\n",
    "            return {\n",
    "                \"content\": f\"No recent headlines found for {ticker}\",\n",
    "                \"sentiment_scores\": []\n",
    "            }\n",
    "        \n",
    "        # Generate comprehensive analysis\n",
    "        analysis = self.generate_analysis(recent_headlines)\n",
    "        #print(\"SENTIMENT ANALYSIS\",analysis)\n",
    "        return {\n",
    "            \"content\": analysis,\n",
    "            \"sentiment_scores\": recent_headlines['Sentiment'].tolist()\n",
    "        }\n",
    "def initialize_firestore(setup_file):\n",
    "    try:\n",
    "        # Check if Firebase apps have already been initialized\n",
    "        if not firebase_admin._apps:\n",
    "            cred = credentials.Certificate(setup_file)\n",
    "            firebase_admin.initialize_app(cred)\n",
    "        else:\n",
    "            print(\"Firebase app already initialized.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    db = firestore.client()\n",
    "    return db\n",
    "class FakeEmbeddings(Embeddings):\n",
    "    def __init__(self, embedding):\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embedding[0] for _ in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embedding[0]\n",
    "# [AnnualReportAnalyzer class implementation remains the same as before]\n",
    "def fetch_financials(ticker):\n",
    "    db = initialize_firestore('secrets_Balaji.json')\n",
    "    docs = db.collection(\"Stock Financial Data\").limit(1).stream()\n",
    "    doc = next(docs, None).to_dict()  # Get the first document from the iterator\n",
    "    return doc[ticker]\n",
    "class AnnualReportAnalyzer:\n",
    "    def __init__(self, ticker, model_name: str = \"llama-3.2-90b-text-preview\", groq_api_key:str = GROQ_API_KEY):\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=groq_api_key,   \n",
    "            temperature=0,\n",
    "            model_name=\"llama-3.2-90b-text-preview\"\n",
    "        )\n",
    "        self.ticker = ticker\n",
    "        self.setup_retriever_from_firebase(ticker)\n",
    "\n",
    "    def setup_retriever_from_firebase(self, ticker):\n",
    "        \"\"\"\n",
    "        Retrieves stock report embeddings from Firebase for a specific ticker\n",
    "        and sets up a retriever.\n",
    "        \n",
    "        Args:\n",
    "            ticker (str): Stock ticker symbol\n",
    "        \"\"\"\n",
    "        # Initialize Firestore client\n",
    "        db = initialize_firestore(\"secrets_Balaji.json\")\n",
    "        \n",
    "        # Get the stock report for the specific ticker\n",
    "        stock_reports = db.collection('Stock_Reports').where(\"stock\", \"==\", ticker).get()\n",
    "        \n",
    "        if not stock_reports:\n",
    "            raise ValueError(f\"No stock report found for ticker {ticker}\")\n",
    "        \n",
    "        # Get the first report data    \n",
    "        report_data = stock_reports[0].to_dict()\n",
    "        \n",
    "        # Create a default embedding if 'embedding' field is missing\n",
    "        if 'embedding' not in report_data:\n",
    "            # Use a simple default embedding (300-dimensional vector of zeros)\n",
    "            default_embedding = [0.0] * 300\n",
    "            embeddings = np.array([default_embedding])\n",
    "            print(f\"Warning: No embedding found for {ticker}. Using default embedding.\")\n",
    "        else:\n",
    "            embeddings = np.array([report_data['embedding']])\n",
    "\n",
    "    # Rest of the method remains the same...\n",
    "        \n",
    "        # Create fake embedding function with our embedding\n",
    "        embedding_function = FakeEmbeddings(embeddings)\n",
    "        \n",
    "        # Get the text chunks\n",
    "        chunk_ids = report_data.get('text_chunks', [])\n",
    "        if not chunk_ids:\n",
    "            print(f\"Warning: No text chunks found for {ticker}\")\n",
    "            # Create a dummy document if no chunks are found\n",
    "            documents = [Document(\n",
    "                page_content=f\"No text chunks available for {ticker}\",\n",
    "                metadata={'stock': ticker, 'chunk_id': 'dummy'}\n",
    "            )]\n",
    "        else:\n",
    "            chunks_ref = db.collection('pdf_text_chunks')\n",
    "            \n",
    "            # Get chunks one by one and create Document objects\n",
    "            documents = []\n",
    "            for chunk_id in chunk_ids:\n",
    "                chunk_doc = chunks_ref.document(chunk_id).get()\n",
    "                if chunk_doc.exists:\n",
    "                    chunk_data = chunk_doc.to_dict()\n",
    "                    documents.append(\n",
    "                        Document(\n",
    "                            page_content=chunk_data.get('chunk', f\"No content for chunk {chunk_id}\"),\n",
    "                            metadata={'stock': ticker, 'chunk_id': chunk_id}\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "        # Create Chroma instance with documents and embedding function\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embedding_function,\n",
    "            persist_directory=f\"./chroma_db_{ticker}\"\n",
    "        )\n",
    "        \n",
    "        # Set up retriever with fewer results if we're using default embeddings\n",
    "        k = 1 if 'embedding' not in report_data else 2\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "        \n",
    "        return self.retriever\n",
    "\n",
    "    def analyze_annual_report(self, query: str) -> str:\n",
    "        try:\n",
    "            system_prompt = \"\"\"You are a financial expert and market analyst. Analyze the annual financial report \n",
    "            of the company and provide a precise summary of the company's performance and investment potential.\n",
    "            \n",
    "            Retrieved Context:\n",
    "            {context}\n",
    "            \n",
    "            If the context is limited or missing, focus on providing general analysis based on the available information.\n",
    "            \"\"\"\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ])\n",
    "            \n",
    "            question_answer_chain = create_stuff_documents_chain(\n",
    "                self.llm, \n",
    "                prompt,\n",
    "                document_variable_name=\"context\"\n",
    "            )\n",
    "            \n",
    "            rag_chain = create_retrieval_chain(\n",
    "                self.retriever,\n",
    "                question_answer_chain\n",
    "            )\n",
    "            \n",
    "            results = rag_chain.invoke({\"input\": query})\n",
    "            #print(\"Annual Report Anlysis\",results[\"answer\"])\n",
    "            return results[\"answer\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in analyze_annual_report: {str(e)}\")\n",
    "            return f\"Unable to analyze annual report for {self.ticker}. Please ensure the required data is available in the database.\"\n",
    "class SectorResearchAgent:\n",
    "    def __init__(self, tavily_api_key: str = \"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\"):\n",
    "        self.tavily_client = TavilyClient(api_key=tavily_api_key)\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=GROQ_API_KEY,\n",
    "            temperature=0,\n",
    "            model_name=\"llama-3.2-90b-text-preview\"\n",
    "        )\n",
    "    \n",
    "    def determine_company_sector(self, ticker: str) -> str:\n",
    "        \"\"\"Research and determine company's sector through web search.\"\"\"\n",
    "        try:\n",
    "            # Search for company sector information\n",
    "            results = self.tavily_client.search(\n",
    "                query=f\"What sector and industry does {ticker} company operate in?\",\n",
    "                search_depth=\"advanced\",\n",
    "                max_results=2\n",
    "            )\n",
    "            \n",
    "            # Extract relevant information from search results\n",
    "            sector_info = \"\"\n",
    "            if isinstance(results, list):\n",
    "                sector_info = \" \".join([r.get('content', '') for r in results if 'content' in r])\n",
    "            elif isinstance(results, dict):\n",
    "                sector_info = results.get('content', '')\n",
    "            \n",
    "            # Use LLM to extract sector from the search results\n",
    "            prompt = f\"\"\"Based on this information about {ticker}, what is the company's main sector? \n",
    "            Information: {sector_info}\n",
    "            \n",
    "            Return ONLY the sector name (e.g., 'Technology', 'Healthcare', 'Finance', etc.) without any additional text or explanation.\"\"\"\n",
    "            \n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            sector = response.content.strip()\n",
    "            \n",
    "            return sector if sector else \"Unknown\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error determining sector for {ticker}: {e}\")\n",
    "            return \"Unknown\"\n",
    "    # Add this method to the SectorResearchAgent class\n",
    "    def research_competitor(self, competitor_ticker: str) -> str:\n",
    "        \"\"\"Research a specific competitor company.\"\"\"\n",
    "        try:\n",
    "            # Search for competitor information using Tavily\n",
    "            queries = [\n",
    "                f\"{competitor_ticker} company performance and market position\",\n",
    "                f\"{competitor_ticker} competitive advantages and strategy\",\n",
    "                f\"{competitor_ticker} recent developments and challenges\"\n",
    "            ]\n",
    "            \n",
    "            findings = []\n",
    "            for query in queries:\n",
    "                results = self.tavily_client.search(\n",
    "                    query=query,\n",
    "                    search_depth=\"advanced\",\n",
    "                    max_results=2\n",
    "                )\n",
    "                \n",
    "                if isinstance(results, list):\n",
    "                    findings.extend(r.get('content', '') for r in results if 'content' in r)\n",
    "                elif isinstance(results, dict):\n",
    "                    findings.append(results.get('content', ''))\n",
    "            \n",
    "            # Use LLM to synthesize findings\n",
    "            if findings:\n",
    "                prompt = f\"\"\"Analyze the following information about {competitor_ticker} and provide a concise summary \n",
    "                of their competitive position, strengths, and challenges:\n",
    "\n",
    "                {' '.join(findings)}\n",
    "\n",
    "                Focus on:\n",
    "                1. Market position\n",
    "                2. Key strengths and weaknesses\n",
    "                3. Recent developments\n",
    "                4. Competitive advantages\n",
    "                \"\"\"\n",
    "                \n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                return response.content\n",
    "            \n",
    "            return f\"No detailed information found for competitor {competitor_ticker}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error researching competitor {competitor_ticker}: {e}\")\n",
    "            return f\"Unable to analyze competitor {competitor_ticker}\"\n",
    "\n",
    "    def research_sector(self, sector_name: str, ticker: str, timeframe: str = \"recent\") -> str:\n",
    "        \"\"\"Conduct research on sector performance and trends.\"\"\"\n",
    "        if sector_name == \"Unknown\":\n",
    "            # If sector is unknown, perform company-specific industry research\n",
    "            queries = [\n",
    "                f\"{ticker} industry analysis and market trends {timeframe}\",\n",
    "                f\"{ticker} competitive landscape and market position {timeframe}\",\n",
    "                f\"{ticker} industry outlook and market dynamics {timeframe}\"\n",
    "            ]\n",
    "        else:\n",
    "            queries = [\n",
    "                f\"Latest {sector_name} sector performance analysis {timeframe}\",\n",
    "                f\"{sector_name} sector outlook and challenges {timeframe}\",\n",
    "                f\"{sector_name} industry trends and market dynamics {timeframe}\"\n",
    "            ]\n",
    "        \n",
    "        findings = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                results = self.tavily_client.search(\n",
    "                    query=query,\n",
    "                    search_depth=\"advanced\",\n",
    "                    max_results=3\n",
    "                )\n",
    "                \n",
    "                if isinstance(results, list):\n",
    "                    for result in results:\n",
    "                        if isinstance(result, dict) and 'content' in result:\n",
    "                            findings.append(result['content'])\n",
    "                elif isinstance(results, dict) and 'content' in results:\n",
    "                    findings.append(results['content'])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sector research: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return \"\\n\\n\".join(findings) if findings else \"No sector research data available.\"\n",
    "\n",
    "class EnhancedComparison:\n",
    "    def __init__(self, ticker, model_name: str = \"llama-3.2-90b-text-preview\", groq_api_key: str = GROQ_API_KEY):\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=groq_api_key,   \n",
    "            temperature=0,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        self.ticker = ticker\n",
    "        self.financial_data = fetch_financials(ticker)\n",
    "        self.sector_researcher = SectorResearchAgent()\n",
    "        \n",
    "    def enhanced_comparison(self, query: str) -> str:\n",
    "        # Get peer comparison data\n",
    "        peer_data = self.financial_data[\"Peer Comparison\"]\n",
    "        rivals = [ticker_ for ticker_ in peer_data if ticker_ != self.ticker]\n",
    "        \n",
    "        # Determine sector through research\n",
    "        sector = self.sector_researcher.determine_company_sector(self.ticker)\n",
    "        print(f\"Detected sector for {self.ticker}: {sector}\")\n",
    "        \n",
    "        # Conduct sector research\n",
    "        sector_research = self.sector_researcher.research_sector(sector, self.ticker)\n",
    "        \n",
    "        # Research each competitor\n",
    "        competitor_research = {}\n",
    "        for rival in rivals[:2]:  # Limit to top 2 competitors to avoid rate limits\n",
    "            competitor_research[rival] = self.sector_researcher.research_competitor(rival)\n",
    "        \n",
    "        # Create enhanced system prompt\n",
    "        system_prompt = (\n",
    "            f\"You are a senior equity research analyst conducting a comprehensive competitive analysis \"\n",
    "            f\"for {self.ticker} in the {sector} sector. \"\n",
    "            f\"Analyze the following data sources to provide a detailed comparison:\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        # Add financial metrics\n",
    "        system_prompt += f\"1. FINANCIAL METRICS\\n{self.ticker}:\\n\"\n",
    "        for metric in peer_data[self.ticker]:\n",
    "            system_prompt += f\"- {metric}: {peer_data[self.ticker][metric]}\\n\"\n",
    "        \n",
    "        system_prompt += \"\\nCompetitor Financials:\\n\"\n",
    "        for rival in rivals:\n",
    "            system_prompt += f\"{rival}:\\n\"\n",
    "            for metric in peer_data[rival]:\n",
    "                system_prompt += f\"- {metric}: {peer_data[rival][metric]}\\n\"\n",
    "            system_prompt += \"\\n\"\n",
    "            \n",
    "        # Add sector research\n",
    "        system_prompt += f\"\\n2. SECTOR ANALYSIS\\n{sector_research}\\n\\n\"\n",
    "        \n",
    "        # Add competitor research\n",
    "        system_prompt += \"3. COMPETITOR INSIGHTS\\n\"\n",
    "        for rival, research in competitor_research.items():\n",
    "            system_prompt += f\"\\n{rival} Analysis:\\n{research}\\n\"\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "        Based on the provided data, create a comprehensive competitive analysis for {self.ticker} in the {sector} sector that includes:\n",
    "        \n",
    "        1. Sector Overview:\n",
    "        - Current sector dynamics and trends\n",
    "        - Key challenges and opportunities\n",
    "        - Regulatory environment\n",
    "        \n",
    "        2. Competitive Position Analysis:\n",
    "        - Market share and positioning\n",
    "        - Relative financial performance\n",
    "        - Competitive advantages/disadvantages\n",
    "        \n",
    "        3. Peer Comparison:\n",
    "        - Detailed financial metrics comparison\n",
    "        - Operational efficiency comparison\n",
    "        - Growth metrics analysis\n",
    "        \n",
    "        4. Forward-Looking Assessment:\n",
    "        - Expected sector developments\n",
    "        - Competitive strategy implications\n",
    "        - Growth opportunities and threats\n",
    "        \n",
    "        Ensure the analysis is data-driven and provides actionable insights for investment decisions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the ChatPromptTemplate\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", analysis_prompt)\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"input\": query})\n",
    "        return result.content\n",
    "\n",
    "def get_stock_price_data(ticker: str) -> dict:\n",
    "    \"\"\"Get current stock price and weekly performance\"\"\"\n",
    "    # Add .NS suffix for Indian stocks\n",
    "    ticker_symbol = f\"{ticker}.NS\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker_symbol)\n",
    "        # Get current data\n",
    "        current_price = stock.info.get('currentPrice', 0)\n",
    "        \n",
    "        # Get historical data for weekly performance\n",
    "        hist = stock.history(period='5d')\n",
    "        week_open = hist['Open'].iloc[0] if not hist.empty else 0\n",
    "        week_change = ((current_price - week_open) / week_open * 100) if week_open != 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"current_price\": current_price,\n",
    "            \"week_open\": week_open,\n",
    "            \"week_change_percent\": round(week_change, 2)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock price data: {e}\")\n",
    "        return {\n",
    "            \"current_price\": 0,\n",
    "            \"week_open\": 0,\n",
    "            \"week_change_percent\": 0\n",
    "        }\n",
    "\n",
    "# Modify the Overview class\n",
    "class Overview:\n",
    "    def __init__(self, ticker, model_name: str = \"llama-3.2-90b-text-preview\", groq_api_key: str = GROQ_API_KEY):\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=groq_api_key,\n",
    "            temperature=0,\n",
    "            model_name=\"llama-3.2-90b-text-preview\"\n",
    "        )\n",
    "        self.ticker = ticker\n",
    "        self.financial_data = fetch_financials(ticker)\n",
    "        self.stock_price_data = get_stock_price_data(ticker)\n",
    "        \n",
    "    def company_report(self, query):\n",
    "        metrics = [metric for metric in self.financial_data if metric not in [\"Peer Comparison\", \"report_url\"]]\n",
    "        \n",
    "        # Add stock price data to system prompt\n",
    "        system_prompt = (\n",
    "            f\"You are an expert market analyst and have to provide a detailed overview of the company based on its \"\n",
    "            f\"financials and current market data.\\n\\n\"\n",
    "            f\"Current Market Data for {self.ticker}:\\n\"\n",
    "            f\"- Current Stock Price: ₹{self.stock_price_data['current_price']}\\n\"\n",
    "            f\"- Weekly Opening Price: ₹{self.stock_price_data['week_open']}\\n\"\n",
    "            f\"- Weekly Performance: {self.stock_price_data['week_change_percent']}%\\n\\n\"\n",
    "            f\"Company Financials:\\n\"\n",
    "        )\n",
    "\n",
    "        for metric in metrics:\n",
    "            system_prompt += f\"{metric}:\\n\"\n",
    "            for sub_metric, value in self.financial_data[metric].items():\n",
    "                system_prompt += f\"--- {sub_metric}: {value}\\n\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"input\": query})\n",
    "        return result.content\n",
    "\n",
    "\n",
    "# Main analysis functions\n",
    "# Modify each function to return with correct state keys\n",
    "def analyze_sentiment(state):\n",
    "    ticker = state[\"ticker\"]\n",
    "    analyzer = NewsAnalyzer()\n",
    "    analysis_result = analyzer.analyze(ticker)\n",
    "    return {\"sentiment_analysis\": analysis_result[\"content\"]}\n",
    "\n",
    "def company_overview_report(state):\n",
    "    ticker = state[\"ticker\"]\n",
    "    overview = Overview(ticker)\n",
    "    company_report = overview.company_report(\n",
    "        f\"Give me a comprehensive overview of {ticker}'s financial performance\"\n",
    "    )\n",
    "    return {\"company_overview_report\": company_report}  # Match state field name\n",
    "\n",
    "def peer_comparison_report(state):\n",
    "    ticker = state[\"ticker\"]\n",
    "    comparison = EnhancedComparison(ticker)\n",
    "    peer_report = comparison.enhanced_comparison(\n",
    "        f\"Provide a competitive analysis of {ticker}\"\n",
    "    )\n",
    "    return {\"peer_comparison_report\": peer_report}\n",
    "\n",
    "def rag_annual_report(state):\n",
    "    ticker = state[\"ticker\"]\n",
    "    analyzer = AnnualReportAnalyzer(ticker)\n",
    "    analysis_result = analyzer.analyze_annual_report(\n",
    "        f\"Analyze {ticker}'s financial performance\"\n",
    "    )\n",
    "    return {\"annual_report_summary\": analysis_result}\n",
    "\n",
    "def generate_equity_report(state):\n",
    "    ticker = state[\"ticker\"]\n",
    "    prompt = report_generator_prompt.format(\n",
    "        ticker=ticker,\n",
    "        sentiment_analysis=state.get(\"sentiment_analysis\", \"\"),\n",
    "        annual_report_summary=state.get(\"annual_report_summary\", \"\"),\n",
    "        company_overview_report=state.get(\"company_overview_report\", \"\"),\n",
    "        peer_comparison_report=state.get(\"peer_comparison_report\", \"\")\n",
    "    )\n",
    "    report = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"equity_research_report\": report.content}\n",
    "\n",
    "\n",
    "### 5. Workflow Graph\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Dict\n",
    "\n",
    "# Define the state structure\n",
    "class GraphState(TypedDict, total=False):\n",
    "    ticker: str\n",
    "    sentiment_analysis: str\n",
    "    annual_report_summary: str\n",
    "    equity_research_report: str\n",
    "    peer_comparison_report: str\n",
    "    company_overview_report: str \n",
    "# Initialize the workflow\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Set up workflow graph\n",
    "workflow.add_node(\"analyze_sentiment\", analyze_sentiment)\n",
    "workflow.add_node(\"company_overview\", company_overview_report)\n",
    "workflow.add_node(\"peer_comparison\", peer_comparison_report)\n",
    "workflow.add_node(\"rag_annual_report\", rag_annual_report)\n",
    "workflow.add_node(\"generate_equity_report\", generate_equity_report)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"analyze_sentiment\")\n",
    "workflow.add_edge(\"analyze_sentiment\", \"company_overview\")\n",
    "workflow.add_edge(\"company_overview\", \"peer_comparison\")\n",
    "workflow.add_edge(\"peer_comparison\", \"rag_annual_report\")\n",
    "workflow.add_edge(\"rag_annual_report\", \"generate_equity_report\")\n",
    "workflow.add_edge(\"generate_equity_report\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# #Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with a ticker\n",
    "    input_state = {\"ticker\": \"HINDUNILVR\"}\n",
    "    \n",
    "    # Run the analysis\n",
    "    print(\"Starting analysis...\")\n",
    "    result = graph.invoke(input_state)\n",
    "    \n",
    "    print(\"\\nFinal Equity Research Report:\")\n",
    "    print(result.get(\"equity_research_report\", \"Report generation failed.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
